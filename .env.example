# Ollama Configuration
OLLAMA_MODEL=mistral:latest
OLLAMA_HOST=http://localhost:11434

# Output Configuration
OUTPUT_DIR=./summaries
MAX_CHUNK_SIZE=4000

# Available models you might want to try:
# mistral:latest, mistral:7b
# llama2, llama2:13b, llama2:70b
# codellama, codellama:13b
# llama2-uncensored
# vicuna